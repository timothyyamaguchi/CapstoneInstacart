{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instacart Capstone Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries and datasets\n",
    "import pandas as pd\n",
    "\n",
    "aisles = pd.read_csv('aisles.csv')\n",
    "departments = pd.read_csv('departments.csv')\n",
    "# order_products_prior = pd.read_csv('order_products_prior.csv')\n",
    "order_products_train = pd.read_csv('order_products_train.csv')\n",
    "orders = pd.read_csv('orders.csv')\n",
    "products = pd.read_csv('products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Indexing the aisles and departments data\n",
    "departments.index = departments.department_id\n",
    "departments = departments.department\n",
    "aisles.index = aisles.aisle_id\n",
    "aisles = aisles.aisle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To create a usable base dataframe without IDs\n",
    "base_products = products.join(aisles, on = 'aisle_id', how = 'right')\n",
    "base_products = base_products.join(departments, on = 'department_id', how = 'right')\n",
    "base_products.index = base_products.product_id\n",
    "base_products = base_products[['product_name', 'aisle', 'department']].sort_index()\n",
    "base_products['organic'] = (base_products['product_name'].str.contains('Organic')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total products are 49688\n",
      "The total organic products are 5035\n"
     ]
    }
   ],
   "source": [
    "# To determine how many organic products there are\n",
    "print \"The total products are %s\" % (len(base_products))\n",
    "organic_products = base_products[base_products['product_name'].str.contains('Organic')]\n",
    "print \"The total organic products are %s\" % (len(organic_products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10069\n"
     ]
    }
   ],
   "source": [
    "# Selecting the train and test dataframe\n",
    "order_num = order_products_train.order_id.unique()\n",
    "order_num = pd.DataFrame(order_num)\n",
    "order_num_exp = order_num.sample(n=1000).sort_index()\n",
    "order_num_exp['order_id'] = order_num_exp.astype(int)\n",
    "order_num_exp['in_test'] = 1\n",
    "order_num_exp.index = order_num_exp.order_id\n",
    "\n",
    "order_products_train.index = order_products_train.order_id\n",
    "\n",
    "order_num_exp = order_num_exp.in_test\n",
    "order_products_exp = order_products_train.join(order_num_exp, on = 'order_id', how = 'right')\n",
    "order_products_exp = order_products_exp[['order_id', 'product_id']]\n",
    "order_products_exp = order_products_exp.join(base_products, on = 'product_id', how = 'left')\n",
    "\n",
    "order_products_exp = order_products_exp.sort_index()\n",
    "order_products_exp = order_products_exp[['product_name', 'aisle', 'department', 'organic']]\n",
    "\n",
    "# must reset the index to parse through data\n",
    "order_products_exp_test = order_products_exp.reset_index(drop = True)\n",
    "\n",
    "# adding a column to combine all variables\n",
    "order_products_exp_test['all_variables'] = order_products_exp_test['product_name'] + order_products_exp_test['aisle'] + order_products_exp_test['department']\n",
    "\n",
    "print len(order_products_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\timothyyamaguchi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Testing the algorithm for aisles\n",
    "# Cleaning the dataframe\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords') # list of words that are generally irrelevant # just downloads the list\n",
    "from nltk.corpus import stopwords # importing the list of stopwords\n",
    "from nltk.stem.porter import PorterStemmer # importing module to stem only the root words\n",
    "corpus = [] # collection of texts\n",
    "for i in range(0,10069): # to loop through all of the rows of the dataframe\n",
    "    review = re.sub('[^a-zA-Z]', ' ', order_products_exp_test['aisle'][i]) # to only keep the letters # need the i to parse through every index\n",
    "    review = review.lower() # to make all of the letters lowercase\n",
    "    review = review.split() # to split the review from string into list of different words\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] # removing all english words that are in the stopwords list # changing for only the stem/root words\n",
    "    review = ' '.join(review) # to join the list of words back into a string\n",
    "    corpus.append(review) # to add to the \"collection of texts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for aisles are 0.425024826216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timothyyamaguchi\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer() # caps the limit of words removing the \"non relevant\" words\n",
    "X = cv.fit_transform(corpus).toarray() # creating a sparse matrix # like a matrics of features\n",
    "y = order_products_exp.iloc[:, 3].values # to take the results of the reviews \n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "accu_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print \"The accuracy score for aisles are %s\" % (accu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\timothyyamaguchi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Testing the algorithm for department\n",
    "# Cleaning the dataframe\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords') # list of words that are generally irrelevant # just downloads the list\n",
    "from nltk.corpus import stopwords # importing the list of stopwords\n",
    "from nltk.stem.porter import PorterStemmer # importing module to stem only the root words\n",
    "corpus = [] # collection of texts\n",
    "for i in range(0,10069): # to loop through all of the rows of the dataframe\n",
    "    review = re.sub('[^a-zA-Z]', ' ', order_products_exp_test['department'][i]) # to only keep the letters # need the i to parse through every index\n",
    "    review = review.lower() # to make all of the letters lowercase\n",
    "    review = review.split() # to split the review from string into list of different words\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] # removing all english words that are in the stopwords list # changing for only the stem/root words\n",
    "    review = ' '.join(review) # to join the list of words back into a string\n",
    "    corpus.append(review) # to add to the \"collection of texts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for departments are 0.37090367428\n"
     ]
    }
   ],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer() # caps the limit of words removing the \"non relevant\" words\n",
    "X = cv.fit_transform(corpus).toarray() # creating a sparse matrix # like a matrics of features\n",
    "y = order_products_exp.iloc[:, 3].values # to take the results of the reviews \n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "accu_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print \"The accuracy score for departments are %s\" % (accu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\timothyyamaguchi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Testing the algorithm for product_name\n",
    "# Cleaning the dataframe\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords') # list of words that are generally irrelevant # just downloads the list\n",
    "from nltk.corpus import stopwords # importing the list of stopwords\n",
    "from nltk.stem.porter import PorterStemmer # importing module to stem only the root words\n",
    "corpus = [] # collection of texts\n",
    "for i in range(0,10069): # to loop through all of the rows of the dataframe\n",
    "    review = re.sub('[^a-zA-Z]', ' ', order_products_exp_test['product_name'][i]) # to only keep the letters # need the i to parse through every index\n",
    "    review = review.lower() # to make all of the letters lowercase\n",
    "    review = review.split() # to split the review from string into list of different words\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] # removing all english words that are in the stopwords list # changing for only the stem/root words\n",
    "    review = ' '.join(review) # to join the list of words back into a string\n",
    "    corpus.append(review) # to add to the \"collection of texts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for product names are 0.990069513406\n"
     ]
    }
   ],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer() # caps the limit of words removing the \"non relevant\" words\n",
    "X = cv.fit_transform(corpus).toarray() # creating a sparse matrix # like a matrics of features\n",
    "y = order_products_exp.iloc[:, 3].values # to take the results of the reviews \n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "accu_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print \"The accuracy score for product names are %s\" % (accu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\timothyyamaguchi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Testing the algorithm for all_variables\n",
    "# Cleaning the dataframe\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords') # list of words that are generally irrelevant # just downloads the list\n",
    "from nltk.corpus import stopwords # importing the list of stopwords\n",
    "from nltk.stem.porter import PorterStemmer # importing module to stem only the root words\n",
    "corpus = [] # collection of texts\n",
    "for i in range(0,10069): # to loop through all of the rows of the dataframe\n",
    "    review = re.sub('[^a-zA-Z]', ' ', order_products_exp_test['all_variables'][i]) # to only keep the letters # need the i to parse through every index\n",
    "    review = review.lower() # to make all of the letters lowercase\n",
    "    review = review.split() # to split the review from string into list of different words\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] # removing all english words that are in the stopwords list # changing for only the stem/root words\n",
    "    review = ' '.join(review) # to join the list of words back into a string\n",
    "    corpus.append(review) # to add to the \"collection of texts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for all variables are 0.72591857001\n"
     ]
    }
   ],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer() # caps the limit of words removing the \"non relevant\" words\n",
    "X = cv.fit_transform(corpus).toarray() # creating a sparse matrix # like a matrics of features\n",
    "y = order_products_exp.iloc[:, 3].values # to take the results of the reviews \n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "accu_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print \"The accuracy score for all variables are %s\" % (accu_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
